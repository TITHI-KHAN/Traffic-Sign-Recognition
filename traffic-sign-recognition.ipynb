{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8391180,"sourceType":"datasetVersion","datasetId":4991320}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This code is a comprehensive set of functions and scripts for performing various image processing and computer vision tasks, particularly focused on traffic sign recognition. \n\nLet's break down the code into sections and provide a brief explanation of each part.\n\n### 1. Importing Libraries and Setting Directories\n```python\nimport cv2\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n- This section imports necessary libraries for image processing, file system operations, data manipulation, and visualization.\n\n### 2. Defining Directory and File Operations\n```python\ndir = \"/kaggle/input/traffic/myData\"\nextracted_files = os.listdir(dir)\n```\n- Sets the directory path where the image data is stored and lists all files in that directory.\n\n### 3. Displaying Sample Images\n```python\ndef display_sample_images(base_path, sample_files, num_samples=4):\n    ...\n```\n- Function to display a few sample images from the dataset.\n\n### 4. Displaying Sample Images Using OpenCV\n```python\ndef display_sample_images_cv(base_path, num_samples=4):\n    ...\n```\n- Similar to the previous function but displays images using OpenCV instead of Matplotlib.\n\n### 5. Displaying Image Histograms\n```python\ndef display_image_histograms(base_path, num_samples=2):\n    ...\n```\n- Displays histograms of pixel intensities for sample images.\n\n### 6. Edge Detection\n```python\ndef display_edge_detection(base_path, num_samples=2):\n    ...\n```\n- Detects edges in sample images using the Canny edge detector.\n\n### 7. Thresholding Images\n```python\ndef display_threshold_images(base_path, num_samples=2):\n    ...\n```\n- Applies adaptive thresholding to sample images.\n\n### 8. Sobel Edge Detection\n```python\ndef display_sobel_edge_images(base_path, num_samples=2):\n    ...\n```\n- Performs Sobel edge detection on sample images.\n\n### 9. Optical Flow\n```python\ndef display_optical_flow_images_corrected(base_path, num_samples=2):\n    ...\n```\n- Computes optical flow between consecutive frames in sample images.\n\n### 10. Watershed Segmentation\n```python\ndef display_watershed_segmentation(base_path, num_samples=2):\n    ...\n```\n- Applies watershed segmentation to sample images.\n\n### 11. Image Inpainting\n```python\ndef display_image_inpainting(base_path, num_samples=2):\n    ...\n```\n- Performs image inpainting on sample images.\n\n### 12. Thermal Effect\n```python\ndef display_thermal_effect(base_path, num_samples=2):\n    ...\n```\n- Applies a thermal effect to sample images.\n\n### 13. Edge Collage\n```python\ndef display_edge_collage(base_path, num_samples=4):\n    ...\n```\n- Creates a collage of images showing various edge detection methods.\n\n### 14. Custom Convolution\n```python\ndef display_custom_convolution(base_path, num_samples=4):\n    ...\n```\n- Applies custom convolution kernels to sample images.\n\n### 15. Creating Label Images\n```python\nlabels_data = pd.read_csv('/kaggle/input/traffic/labels.csv')\n```\n- Reads label data from a CSV file containing traffic sign labels.\n\n### 16. Loading Image Data\n```python\ndef load_data(data_directory, target_size=(64, 64)):\n    ...\n```\n- Function to load image data from directories, preprocess them, and split into training and test sets.\n\n### 17. Creating a CNN Model\n```python\ndef create_model(input_shape, num_classes):\n    ...\n```\n- Defines a convolutional neural network (CNN) model architecture for traffic sign classification.\n\n### 18. Compiling and Training the Model\n```python\ninput_shape = (64, 64, 3)\nnum_classes = 43\nmodel = create_model(input_shape, num_classes)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_images, train_labels, epochs=10, validation_split=0.2)\n```\n- Compiles the model, specifying optimizer, loss function, and evaluation metrics, then trains the model on the training data.\n\n### 19. Evaluating Model Performance\n```python\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n```\n- Evaluates the trained model on the test dataset and prints the test accuracy.\n\n### 20. Plotting Training History\n```python\nif 'history' in globals():\n    ...\n```\n- Plots the training and validation accuracy and loss over epochs.\n\n### 21. Plotting Predictions\n```python\ndef plot_images(images, actual_labels, predicted_labels, class_names):\n    ...\n```\n- Function to plot sample images with their predicted and actual labels.\n\n### 22. Fine-Tuning a Pretrained Model\n```python\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n...\n```\n- Loads a pretrained MobileNetV2 model, freezes its layers, and adds custom classification layers on top.\n\n### 23. Compiling and Training the Fine-Tuned Model\n```python\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n```\n- Compiles and trains the fine-tuned model.\n\n### 24. Plotting Predictions for Fine-Tuned Model\n```python\npredictions = model.predict(test_images)\npredicted_labels = np.argmax(predictions, axis=1)\nactual_labels = np.argmax(test_labels, axis=1) if test_labels.ndim > 1 else test_labels\nplot_images(test_images, actual_labels, predicted_labels, class_names)\n```\n- Plots predictions for the fine-tuned model.\n\nThis code provides a comprehensive workflow for image processing, model training, and evaluation, particularly focused on traffic sign recognition. Each section performs a specific task, such as data preprocessing, model definition, training, evaluation, and visualization.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"**Code**","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:20:44.642044Z","iopub.execute_input":"2024-05-12T12:20:44.642452Z","iopub.status.idle":"2024-05-12T12:20:44.647480Z","shell.execute_reply.started":"2024-05-12T12:20:44.642423Z","shell.execute_reply":"2024-05-12T12:20:44.646383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir = \"/kaggle/input/traffic/myData\"","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:20:44.648947Z","iopub.execute_input":"2024-05-12T12:20:44.649183Z","iopub.status.idle":"2024-05-12T12:20:44.656992Z","shell.execute_reply.started":"2024-05-12T12:20:44.649162Z","shell.execute_reply":"2024-05-12T12:20:44.656102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extracted_files = os.listdir(dir)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:20:44.657988Z","iopub.execute_input":"2024-05-12T12:20:44.658244Z","iopub.status.idle":"2024-05-12T12:20:44.676216Z","shell.execute_reply.started":"2024-05-12T12:20:44.658223Z","shell.execute_reply":"2024-05-12T12:20:44.675538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n\n\ndef display_sample_images(base_path, sample_files, num_samples=4):\n    plt.figure(figsize=(12, 3))\n    for i, file in enumerate(sample_files[:num_samples]):\n        img_path = os.path.join(base_path, file)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n        \n        plt.subplot(1, num_samples, i+1)\n        plt.imshow(img)\n        plt.title(file.split('/')[-1])\n        plt.axis('off')\n    plt.show()\n\n\nsample_folder = 'myData/1/'\nsample_files = [f for f in extracted_files if sample_folder in f and f.endswith('.jpg')]\n\ndisplay_sample_images(dir, sample_files, num_samples=4)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:20:44.677719Z","iopub.execute_input":"2024-05-12T12:20:44.677970Z","iopub.status.idle":"2024-05-12T12:20:44.688304Z","shell.execute_reply.started":"2024-05-12T12:20:44.677949Z","shell.execute_reply":"2024-05-12T12:20:44.687340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_sample_images_cv(base_path, num_samples=4):\n    file_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(base_path) for f in filenames if f.endswith('.jpg')]\n    plt.figure(figsize=(12, 3))\n    for i, file in enumerate(file_paths[:num_samples]):\n        img = cv2.imread(file)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n        \n        plt.subplot(1, num_samples, i+1)\n        plt.imshow(img)\n        plt.title(file.split('/')[-1])\n        plt.axis('off')\n    plt.show()\n\ndisplay_sample_images_cv(dir, num_samples=4)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:20:44.689369Z","iopub.execute_input":"2024-05-12T12:20:44.689714Z","iopub.status.idle":"2024-05-12T12:21:14.927998Z","shell.execute_reply.started":"2024-05-12T12:20:44.689682Z","shell.execute_reply":"2024-05-12T12:21:14.927029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_image_histograms(base_path, num_samples=2):\n    file_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(base_path) for f in filenames if f.endswith('.jpg')]\n    #print(file_paths)\n    plt.figure(figsize=(12, 5 * num_samples))\n    \n    for i, file in enumerate(file_paths[:num_samples]):\n        print(\"File Name:\",file)\n        img = cv2.imread(file)\n        colors = ('b', 'g', 'r')\n        \n        plt.subplot(num_samples, 2, 2*i+1)\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        plt.title('Image')\n        plt.axis('off')\n        \n        plt.subplot(num_samples, 2, 2*i+2)\n        for j, color in enumerate(colors):\n            hist = cv2.calcHist([img], [j], None, [256], [0, 256])\n            plt.plot(hist, color=color)\n            plt.xlim([0, 256])\n        plt.title('Histogram')\n    \n    plt.tight_layout()\n    plt.show()\n\ndisplay_image_histograms(dir, num_samples=2)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:21:14.929379Z","iopub.execute_input":"2024-05-12T12:21:14.929647Z","iopub.status.idle":"2024-05-12T12:21:15.882365Z","shell.execute_reply.started":"2024-05-12T12:21:14.929613Z","shell.execute_reply":"2024-05-12T12:21:15.881464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_edge_detection(base_path, num_samples=2):\n    file_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(base_path) for f in filenames if f.endswith('.jpg')]\n    plt.figure(figsize=(12, 3 * num_samples))\n    \n    for i, file in enumerate(file_paths[:num_samples]):\n        img = cv2.imread(file)\n        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        edges = cv2.Canny(gray_img, 100, 200)\n\n        plt.subplot(num_samples, 2, 2*i+1)\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        plt.title('Original Image')\n        plt.axis('off')\n\n        plt.subplot(num_samples, 2, 2*i+2)\n        plt.imshow(edges, cmap='gray')\n        plt.title('Edge Detection')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndisplay_edge_detection(dir, num_samples=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:21:15.884459Z","iopub.execute_input":"2024-05-12T12:21:15.884758Z","iopub.status.idle":"2024-05-12T12:21:16.637035Z","shell.execute_reply.started":"2024-05-12T12:21:15.884733Z","shell.execute_reply":"2024-05-12T12:21:16.636077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_threshold_images(base_path, num_samples=2):\n    file_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(base_path) for f in filenames if f.endswith('.jpg')]\n    plt.figure(figsize=(12, 3 * num_samples))\n    \n    for i, file in enumerate(file_paths[:num_samples]):\n        img = cv2.imread(file)\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        adaptive_thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2) # Sajib BHaia\n\n        plt.subplot(num_samples, 2, 2*i+1)\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        plt.title('Original Image')\n        plt.axis('off')\n\n        plt.subplot(num_samples, 2, 2*i+2)\n        plt.imshow(adaptive_thresh, cmap='gray')\n        plt.title('Adaptive Threshold Image')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndisplay_threshold_images(dir, num_samples=2)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:21:16.638065Z","iopub.execute_input":"2024-05-12T12:21:16.638323Z","iopub.status.idle":"2024-05-12T12:21:17.412681Z","shell.execute_reply.started":"2024-05-12T12:21:16.638301Z","shell.execute_reply":"2024-05-12T12:21:17.411785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef display_sobel_edge_images(base_path, num_samples=2):\n    file_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(base_path) for f in filenames if f.endswith('.jpg')]\n    plt.figure(figsize=(12, 3 * num_samples))\n    \n    for i, file in enumerate(file_paths[:num_samples]):\n        img = cv2.imread(file)\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)  # Sobel Edge Detection on the X axis # Mahmudul BHaia\n        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)  # Sobel Edge Detection on the Y axis\n\n        plt.subplot(num_samples, 3, 3*i+1)\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        plt.title('Original Image')\n        plt.axis('off')\n\n        plt.subplot(num_samples, 3, 3*i+2)\n        plt.imshow(sobelx, cmap='gray')\n        plt.title('Sobel X Image')\n        plt.axis('off')\n\n        plt.subplot(num_samples, 3, 3*i+3)\n        plt.imshow(sobely, cmap='gray')\n        plt.title('Sobel Y Image')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndisplay_sobel_edge_images(dir, num_samples=2)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:21:17.414051Z","iopub.execute_input":"2024-05-12T12:21:17.414425Z","iopub.status.idle":"2024-05-12T12:21:18.321236Z","shell.execute_reply.started":"2024-05-12T12:21:17.414390Z","shell.execute_reply":"2024-05-12T12:21:18.320296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_sobel_edge_images(base_path, num_samples=2):\n    file_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(base_path) for f in filenames if f.endswith('.jpg')]\n    plt.figure(figsize=(12, 3 * num_samples))\n    \n    for i, file in enumerate(file_paths[:num_samples]):\n        img = cv2.imread(file)\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)  # Sobel Edge Detection on the X axis\n        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)  # Sobel Edge Detection on the Y axis\n\n        plt.subplot(num_samples, 3, 3*i+1)\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        plt.title('Original Image')\n        plt.axis('off')\n\n        plt.subplot(num_samples, 3, 3*i+2)\n        plt.imshow(sobelx, cmap='gray')\n        plt.title('Sobel X Image')\n        plt.axis('off')\n\n        plt.subplot(num_samples, 3, 3*i+3)\n        plt.imshow(sobely, cmap='gray')\n        plt.title('Sobel Y Image')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndisplay_sobel_edge_images(dir, num_samples=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:21:18.322428Z","iopub.execute_input":"2024-05-12T12:21:18.322723Z","iopub.status.idle":"2024-05-12T12:21:19.219263Z","shell.execute_reply.started":"2024-05-12T12:21:18.322698Z","shell.execute_reply":"2024-05-12T12:21:19.218366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_optical_flow_images_corrected(base_path, num_samples=2):\n    file_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(base_path) for f in filenames if f.endswith('.jpg')]\n    plt.figure(figsize=(12, 3 * num_samples))\n    \n    lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n\n    for i in range(num_samples - 1):\n        old_frame = cv2.imread(file_paths[i])\n        frame = cv2.imread(file_paths[i + 1])\n        mask = np.zeros_like(old_frame)\n\n        old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n        p0 = cv2.goodFeaturesToTrack(old_gray, maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n        if p0 is not None and len(p0) > 0:\n            p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n            good_new = p1[st == 1]\n            good_old = p0[st == 1]\n\n            for new, old in zip(good_new, good_old):\n                a, b = new.ravel()\n                c, d = old.ravel()\n                mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), (255, 0, 0), 2)\n                frame = cv2.circle(frame, (int(a), int(b)), 5, (0, 255, 0), -1)\n\n            img = cv2.add(frame, mask)\n\n            plt.subplot(num_samples - 1, 2, 2*i+1)\n            plt.imshow(cv2.cvtColor(old_frame, cv2.COLOR_BGR2RGB))\n            plt.title('Previous Frame')\n            plt.axis('off')\n\n            plt.subplot(num_samples - 1, 2, 2*i+2) \n            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n            plt.title('Optical Flow')\n            plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\ndisplay_optical_flow_images_corrected(dir, num_samples=3)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:21:19.220546Z","iopub.execute_input":"2024-05-12T12:21:19.220858Z","iopub.status.idle":"2024-05-12T12:21:20.022067Z","shell.execute_reply.started":"2024-05-12T12:21:19.220831Z","shell.execute_reply":"2024-05-12T12:21:20.021173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_watershed_segmentation(base_path, num_samples=2):\n    file_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(base_path) for f in filenames if f.endswith('.jpg')]\n    plt.figure(figsize=(12, 3 * num_samples))\n    \n    for i, file in enumerate(file_paths[:num_samples]):\n        img = cv2.imread(file)\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n        kernel = np.ones((3,3),np.uint8)\n        opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations = 2)\n        sure_bg = cv2.dilate(opening, kernel, iterations=3)\n\n\n        dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n        ret, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n\n\n        sure_fg = np.uint8(sure_fg)\n        unknown = cv2.subtract(sure_bg, sure_fg)\n\n        ret, markers = cv2.connectedComponents(sure_fg)\n        \n        markers = markers+1\n        \n        markers[unknown == 255] = 0\n\n        markers = cv2.watershed(img, markers)\n        img[markers == -1] = [255, 0, 0]  # Marking boundary in red\n\n        plt.subplot(num_samples, 2, 2*i+1)\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        plt.title('Segmented Image')\n        plt.axis('off')\n\n        plt.subplot(num_samples, 2, 2*i+2)\n        plt.imshow(markers, cmap='jet')  # Visualize the marker image\n        plt.title('Watershed Markers')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndisplay_watershed_segmentation(dir, num_samples=4)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:21:20.023302Z","iopub.execute_input":"2024-05-12T12:21:20.023610Z","iopub.status.idle":"2024-05-12T12:21:21.426024Z","shell.execute_reply.started":"2024-05-12T12:21:20.023583Z","shell.execute_reply":"2024-05-12T12:21:21.425107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef display_image_inpainting(base_path, num_samples=2):\n    file_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(base_path) for f in filenames if f.endswith('.jpg')]\n    plt.figure(figsize=(12, 3 * num_samples))\n    \n    for i, file in enumerate(file_paths[:num_samples]):\n        img = cv2.imread(file)\n        mask = np.zeros(img.shape[:2], np.uint8)\n\n        mask[20:120, 20:120] = 1\n\n        inpainted = cv2.inpaint(img, mask, 3, cv2.INPAINT_TELEA)\n\n        plt.subplot(num_samples, 2, 2*i+1)\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        plt.title('Original Image')\n        plt.axis('off')\n\n        plt.subplot(num_samples, 2, 2*i+2)\n        plt.imshow(cv2.cvtColor(inpainted, cv2.COLOR_BGR2RGB))\n        plt.title('Inpainted Image')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndisplay_image_inpainting(dir, num_samples=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:21:21.427296Z","iopub.execute_input":"2024-05-12T12:21:21.427682Z","iopub.status.idle":"2024-05-12T12:21:22.162890Z","shell.execute_reply.started":"2024-05-12T12:21:21.427648Z","shell.execute_reply":"2024-05-12T12:21:22.161933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_thermal_effect(base_path, num_samples=2):\n    file_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(base_path) for f in filenames if f.endswith('.jpg')]\n    plt.figure(figsize=(12, 3 * num_samples))\n    \n    for i, file in enumerate(file_paths[:num_samples]):\n        img = cv2.imread(file)\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n        # Applying a thermal effect using pseudocolor\n        thermal = cv2.applyColorMap(gray, cv2.COLORMAP_JET)\n\n        plt.subplot(num_samples, 2, 2*i+1)\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        plt.title('Original Image')\n        plt.axis('off')\n\n        plt.subplot(num_samples, 2, 2*i+2)\n        plt.imshow(thermal)\n        plt.title('Thermal Effect')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndisplay_thermal_effect(dir, num_samples=2)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:21:22.166226Z","iopub.execute_input":"2024-05-12T12:21:22.166485Z","iopub.status.idle":"2024-05-12T12:21:22.906753Z","shell.execute_reply.started":"2024-05-12T12:21:22.166463Z","shell.execute_reply":"2024-05-12T12:21:22.905788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_edge_collage(base_path, num_samples=4):\n    file_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(base_path) for f in filenames if f.endswith('.jpg')]\n    plt.figure(figsize=(12, 6))  # Larger figure size to accommodate more images\n    \n    for i, file in enumerate(file_paths[:num_samples]):\n        img = cv2.imread(file)\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)\n        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)\n\n        edges = cv2.Canny(gray, 100, 200)\n\n        combined = np.hstack((sobelx, sobely, edges))\n\n        plt.subplot(2, num_samples, i + 1)\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        plt.title(f'Original Image {i+1}')\n        plt.axis('off')\n\n        plt.subplot(2, num_samples, i + num_samples + 1)\n        plt.imshow(combined, cmap='gray')\n        plt.title(f'Edges Combined {i+1}')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndisplay_edge_collage(dir, num_samples=4)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:21:22.907960Z","iopub.execute_input":"2024-05-12T12:21:22.908248Z","iopub.status.idle":"2024-05-12T12:21:23.907573Z","shell.execute_reply.started":"2024-05-12T12:21:22.908223Z","shell.execute_reply":"2024-05-12T12:21:23.906680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_custom_convolution(base_path, num_samples=4):\n    file_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(base_path) for f in filenames if f.endswith('.jpg')]\n    plt.figure(figsize=(12, 6))  # Adjusted figure size for more images and clearer display\n\n    edge_kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype=np.float32)  # Edge enhancement\n    sharpen_kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], dtype=np.float32)  # Sharpening\n    emboss_kernel = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]], dtype=np.float32)  # Emboss\n\n    for i, file in enumerate(file_paths[:num_samples]):\n        img = cv2.imread(file)\n\n        # Apply custom convolution kernels\n        edge_enhanced = cv2.filter2D(img, -1, edge_kernel)\n        sharpened = cv2.filter2D(img, -1, sharpen_kernel)\n        embossed = cv2.filter2D(img, -1, emboss_kernel)\n\n        plt.subplot(num_samples, 4, 4*i+1)\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        plt.title('Original Image')\n        plt.axis('off')\n\n        plt.subplot(num_samples, 4, 4*i+2)\n        plt.imshow(cv2.cvtColor(edge_enhanced, cv2.COLOR_BGR2RGB))\n        plt.title('Edge Enhancement')\n        plt.axis('off')\n\n        plt.subplot(num_samples, 4, 4*i+3)\n        plt.imshow(cv2.cvtColor(sharpened, cv2.COLOR_BGR2RGB))\n        plt.title('Sharpening')\n        plt.axis('off')\n\n        plt.subplot(num_samples, 4, 4*i+4)\n        plt.imshow(cv2.cvtColor(embossed, cv2.COLOR_BGR2RGB))\n        plt.title('Emboss')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndisplay_custom_convolution(dir, num_samples=4)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:21:23.908795Z","iopub.execute_input":"2024-05-12T12:21:23.909056Z","iopub.status.idle":"2024-05-12T12:21:25.122171Z","shell.execute_reply.started":"2024-05-12T12:21:23.909033Z","shell.execute_reply":"2024-05-12T12:21:25.121277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_data = pd.read_csv('/kaggle/input/traffic/labels.csv')\n\ndef create_label_image(text, width=200, height=60, font_scale=1, thickness=2):\n    image = np.ones((height, width, 3), dtype=\"uint8\") * 255  # white background\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, thickness)\n    text_offset_x = (width - text_width) // 2\n    text_offset_y = (height + text_height) // 2\n    cv2.putText(image, text, (text_offset_x, text_offset_y), font, font_scale, (0, 0, 0), thickness)\n    return image\n\nlabel_images = [create_label_image(row['Name']) for _, row in labels_data.iterrows()]\n\n\nfig, axs = plt.subplots(3, 4, figsize=(15, 10))\nfor ax, img in zip(axs.flat, label_images):\n    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:21:25.123479Z","iopub.execute_input":"2024-05-12T12:21:25.124138Z","iopub.status.idle":"2024-05-12T12:21:25.584044Z","shell.execute_reply.started":"2024-05-12T12:21:25.124107Z","shell.execute_reply":"2024-05-12T12:21:25.582942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\n\ndef load_data(data_directory, target_size=(64, 64)):\n    images = []\n    labels = []\n    \n    for label in os.listdir(data_directory):\n        label_path = os.path.join(data_directory, label)\n        if os.path.isdir(label_path):\n            for image_file in os.listdir(label_path):\n                image_path = os.path.join(label_path, image_file)\n                image = cv2.imread(image_path)\n                if image is not None:\n                    image = cv2.resize(image, target_size)\n                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n                    images.append(image)\n                    labels.append(int(label))\n    \n    images = np.array(images, dtype='float32')\n    labels = np.array(labels, dtype='int32')\n    \n    # Normalize the image data to 0-1\n    images /= 255.0\n    \n    # Split into training and test sets\n    train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n    \n    # Convert labels to one-hot encoding\n    train_labels = to_categorical(train_labels)\n    test_labels = to_categorical(test_labels)\n    \n    return (train_images, train_labels), (test_images, test_labels)\n\n\ndata_directory = '/kaggle/input/traffic/myData'  \n(train_images, train_labels), (test_images, test_labels) = load_data(data_directory)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:21:25.585394Z","iopub.execute_input":"2024-05-12T12:21:25.585872Z","iopub.status.idle":"2024-05-12T12:22:56.546069Z","shell.execute_reply.started":"2024-05-12T12:21:25.585838Z","shell.execute_reply":"2024-05-12T12:22:56.545256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\ndef create_model(input_shape, num_classes):\n    model = Sequential([\n        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n        MaxPooling2D((2, 2)),\n        Conv2D(64, (3, 3), activation='relu'),\n        MaxPooling2D((2, 2)),\n        Conv2D(64, (3, 3), activation='relu'),\n        Flatten(),\n        Dense(64, activation='relu'),\n        Dropout(0.5),\n        Dense(num_classes, activation='softmax')\n    ])\n    \n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n\ninput_shape = (64, 64, 3)  \nnum_classes = 43  \n\nmodel = create_model(input_shape, num_classes)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:22:56.547278Z","iopub.execute_input":"2024-05-12T12:22:56.547623Z","iopub.status.idle":"2024-05-12T12:22:56.622577Z","shell.execute_reply.started":"2024-05-12T12:22:56.547592Z","shell.execute_reply":"2024-05-12T12:22:56.621753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='categorical_crossentropy',  \n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:22:56.623551Z","iopub.execute_input":"2024-05-12T12:22:56.623826Z","iopub.status.idle":"2024-05-12T12:22:56.632076Z","shell.execute_reply.started":"2024-05-12T12:22:56.623803Z","shell.execute_reply":"2024-05-12T12:22:56.631007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_images, train_labels, epochs=10, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:22:56.633167Z","iopub.execute_input":"2024-05-12T12:22:56.633453Z","iopub.status.idle":"2024-05-12T12:24:10.474144Z","shell.execute_reply.started":"2024-05-12T12:22:56.633430Z","shell.execute_reply":"2024-05-12T12:24:10.473083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_images, test_labels)\nprint(f\"Test Accuracy: {test_acc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:24:10.475907Z","iopub.execute_input":"2024-05-12T12:24:10.476288Z","iopub.status.idle":"2024-05-12T12:24:13.347647Z","shell.execute_reply.started":"2024-05-12T12:24:10.476254Z","shell.execute_reply":"2024-05-12T12:24:13.346707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Check if 'history' has been defined\nif 'history' in globals():\n    # Plot training & validation accuracy values\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model Accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n\n    # Plot training & validation loss values\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model Loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"The training history is not available. Please train your model with 'history = model.fit(...)' and make sure 'history' is accessible.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:24:13.349029Z","iopub.execute_input":"2024-05-12T12:24:13.349397Z","iopub.status.idle":"2024-05-12T12:24:13.839311Z","shell.execute_reply.started":"2024-05-12T12:24:13.349355Z","shell.execute_reply":"2024-05-12T12:24:13.838339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = list(labels_data['Name'])","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:24:13.840789Z","iopub.execute_input":"2024-05-12T12:24:13.841180Z","iopub.status.idle":"2024-05-12T12:24:13.846139Z","shell.execute_reply.started":"2024-05-12T12:24:13.841146Z","shell.execute_reply":"2024-05-12T12:24:13.845072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_images(images, actual_labels, predicted_labels, class_names):\n    n = 12  \n    indexes = np.random.choice(len(images), n, replace=False)\n    \n    plt.figure(figsize=(15, 8))\n    for i, index in enumerate(indexes):\n        ax = plt.subplot(3, 4, i + 1)\n        plt.imshow(images[index])\n        plt.title(f\"Predicted: {class_names[predicted_labels[index]]}\\nActual: {class_names[actual_labels[index]]}\")\n        plt.axis(\"off\")\n\n#class_names = ['Speed Limit', 'Yield', 'Stop', 'No Entry', 'Traffic Signal', 'Pedestrian', 'Children Crossing', 'Cycle Path', 'Animal Crossing', 'No Parking']\n\npredictions = model.predict(test_images)\npredicted_labels = np.argmax(predictions, axis=1)\n\n\nactual_labels = np.argmax(test_labels, axis=1) if test_labels.ndim > 1 else test_labels\n\nplot_images(test_images, actual_labels, predicted_labels, class_names)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:24:13.847482Z","iopub.execute_input":"2024-05-12T12:24:13.847934Z","iopub.status.idle":"2024-05-12T12:24:18.448427Z","shell.execute_reply.started":"2024-05-12T12:24:13.847902Z","shell.execute_reply":"2024-05-12T12:24:18.447572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n\nbase_model.trainable = False\n\ninputs = tf.keras.Input(shape=(64, 64, 3))\nx = base_model(inputs, training=False)\nx = GlobalAveragePooling2D()(x)\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.5)(x)\noutputs = Dense(43, activation='softmax')(x)  # Assuming 43 classes for traffic signs\n\nmodel = Model(inputs, outputs)\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:24:18.449504Z","iopub.execute_input":"2024-05-12T12:24:18.449775Z","iopub.status.idle":"2024-05-12T12:24:20.041525Z","shell.execute_reply.started":"2024-05-12T12:24:18.449753Z","shell.execute_reply":"2024-05-12T12:24:20.040624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:24:20.042777Z","iopub.execute_input":"2024-05-12T12:24:20.043418Z","iopub.status.idle":"2024-05-12T12:27:05.522155Z","shell.execute_reply.started":"2024-05-12T12:24:20.043374Z","shell.execute_reply":"2024-05-12T12:27:05.521081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_images(images, actual_labels, predicted_labels, class_names):\n    n = 12  \n    indexes = np.random.choice(len(images), n, replace=False)\n    \n    plt.figure(figsize=(15, 8))\n    for i, index in enumerate(indexes):\n        ax = plt.subplot(3, 4, i + 1)\n        plt.imshow(images[index])\n        plt.title(f\"Predicted: {class_names[predicted_labels[index]]}\\nActual: {class_names[actual_labels[index]]}\")\n        plt.axis(\"off\")\n\n#class_names = ['Speed Limit', 'Yield', 'Stop', 'No Entry', 'Traffic Signal', 'Pedestrian', 'Children Crossing', 'Cycle Path', 'Animal Crossing', 'No Parking']\n\npredictions = model.predict(test_images)\npredicted_labels = np.argmax(predictions, axis=1)\n\n\nactual_labels = np.argmax(test_labels, axis=1) if test_labels.ndim > 1 else test_labels\n\nplot_images(test_images, actual_labels, predicted_labels, class_names)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T12:27:05.524130Z","iopub.execute_input":"2024-05-12T12:27:05.524436Z","iopub.status.idle":"2024-05-12T12:27:15.528265Z","shell.execute_reply.started":"2024-05-12T12:27:05.524410Z","shell.execute_reply":"2024-05-12T12:27:15.527344Z"},"trusted":true},"execution_count":null,"outputs":[]}]}